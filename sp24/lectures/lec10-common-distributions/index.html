<!DOCTYPE html><html lang="en-US"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="shortcut icon" href="/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script type="text/javascript" src="/assets/js/vendor/lunr.min.js"></script> <script type="text/javascript" src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><title>Lecture 10 - Common Probability Distributions | Stat 157/260</title><meta name="generator" content="Jekyll v3.9.0" /><meta property="og:title" content="Lecture 10 - Common Probability Distributions" /><meta name="author" content="Jean-Stanislas Denain" /><meta property="og:locale" content="en_US" /><meta name="description" content="Website for the UC Berkeley class STAT 165 / STAT 265." /><meta property="og:description" content="Website for the UC Berkeley class STAT 165 / STAT 265." /><link rel="canonical" href="http://forecastingclass.com/lectures/lec10-common-distributions/" /><meta property="og:url" content="http://forecastingclass.com/lectures/lec10-common-distributions/" /><meta property="og:site_name" content="Stat 157/260" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Lecture 10 - Common Probability Distributions" /> <script type="application/ld+json"> {"url":"http://forecastingclass.com/lectures/lec10-common-distributions/","headline":"Lecture 10 - Common Probability Distributions","author":{"@type":"Person","name":"Jean-Stanislas Denain"},"description":"Website for the UC Berkeley class STAT 165 / STAT 265.","@type":"WebPage","@context":"https://schema.org"}</script><body> <svg xmlns="http://www.w3.org/2000/svg" style="display: none;"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> </svg><div class="side-bar"><div class="site-header"> <a href="http://forecastingclass.com/" class="site-title lh-tight"> Stat 157/260 </a> <a href="#" id="menu-button" class="site-button"> <svg viewBox="0 0 24 24" class="icon"><use xlink:href="#svg-menu"></use></svg> </a></div><nav role="navigation" aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"><a href="http://forecastingclass.com/" class="nav-list-link">About</a><li class="nav-list-item"><a href="http://forecastingclass.com/calendar/" class="nav-list-link">Calendar</a></ul></nav><footer class="site-footer"> This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Stat 157/260" aria-label="Search Stat 157/260" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div></div><div id="main-content-wrap" class="main-content-wrap"><div id="main-content" class="main-content" role="main"> <script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"> MathJax.Hub.Config({ tex2jax: { inlineMath: [["$", "$"], ["\\(", "\\)"]], processEscapes: true } }); </script><h1 id="lecture-10-common-probability-distributions"> <a href="#lecture-10-common-probability-distributions" class="anchor-heading" aria-labelledby="lecture-10-common-probability-distributions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Lecture 10: Common Probability Distributions</h1><p>When we output a forecast, we're either explicitly or implicitly outputting a <em>probability distribution</em>.</p><p>For example, if we forecast the AQI in Berkeley tomorrow to be “around” 30, plus or minus 10, we implicitly mean some distribution that has most of its probability mass between 20 and 40. If we were forced to be explicit, we might say we have a normal distribution with mean 30 and standard deviation 10 in mind.</p><p>There are many different types of probability distributions, so it's helpful to know what shapes distributions tend to have and what factors influence this.</p><p>From your math and probability classes, you're probability used to the Gaussian or normal distribution as the “canonical” example of a probability distribution. However, in practice other distributions are much more common. While normal distributions do show up, it's more common to see distributions such as <strong>log-normal</strong> or <strong>power law</strong> distributions.</p><p>In the remainder of these notes, I'll discuss each of these in turn. The following table summarizes these distributions, what typically causes them to occur, and several examples of data that follow the distribution: <br /></p><div class="table-wrapper"><table><thead><tr><th>Distribution<th>Gaussian<th>Log-normal<th>Power Law<tbody><tr><td>Causes<td>Independent <em>additive</em> factors<td>Independent <em>multiplicative</em> factors<td>Rich get richer, scale invariance<tr><td>Tails<td>Thin tails<td>Heavy tails<td>Heavier tails<tr><td>Examples<td>-heights<td>-US GDP in 2030<td>-city population<tr><td> <td>-temperature<td>-price of Tesla stock in 2030<td>-twitter followers<tr><td> <td>-measurement errors<td> <td>-word frequencies</table></div><h2 id="normal-distribution"> <a href="#normal-distribution" class="anchor-heading" aria-labelledby="normal-distribution"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Normal Distribution</h2><p>The normal (or Gaussian) distribution is the familiar “bell-shaped” curve seen in many textbooks. Its probability density is given by $p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\Big(-\frac{(x-\mu)^2}{2\sigma^2}\Big)$, where $\mu$ is the mean and $\sigma$ is the standard deviation.</p><p><img src="https://bounded-regret.ghost.io/content/images/2021/11/normal_dist-1.png" alt="normal_dist-1" /></p><p>Normal distributions occurs when there are many independent factors that combine additively, and no single one of those factors “dominates” the sum. Mathematically, this intuition is formalized through the <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">central limit theorem</a>.</p><p><strong>Example 1: temperature.</strong> As one example, the temperature in a given city (at a given time of year) is normally distributed, since many factors (wind, ocean currents, cloud cover, pollution) affect it, mostly independently.</p><p align="center"> <img src="https://bounded-regret.ghost.io/content/images/2021/12/temps.png" /></p><p><strong>Example 2: heights.</strong> Similarly, height is normally distributed, since many different genes have some effect on height, as do other factors such as childhood nutrition.</p><p>However, for height we actually have to be careful, because there are two major factors that affect height significantly: age and sex. 12-year olds are (generally) shorter than 22-year-olds, and women are on average 5 inches (13cm) shorter than men. These overlaid histograms show heights of adults conditional on sex.</p><p align="center"> <img src="https://bounded-regret.ghost.io/content/images/2021/11/height_hist_summary.png" /></p><p>Thus, if we try to approximate the distribution of heights of all adults with a normal distribution, we will get a pretty bad approximation. However, the distribution of male heights and female heights are separately well-approximated by normal distributions.</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">All<th style="text-align: center">Males<th style="text-align: center">Females<tbody><tr><td style="text-align: center"><img src="https://bounded-regret.ghost.io/content/images/2021/11/height_normal_all.png" alt="height_normal_all" /><td style="text-align: center"><img src="https://bounded-regret.ghost.io/content/images/2021/11/height_normal_male.png" alt="height_normal_male" /><td style="text-align: center"><img src="https://bounded-regret.ghost.io/content/images/2021/11/height_normal_female.png" alt="height_normal_female" /></table></div><p><strong>Example 3: measurement errors.</strong> Finally, the errors of a well-engineered system are often normally-distributed. One example would be a physical measurement apparatus (such as a <a href="https://en.wikipedia.org/wiki/Voltmeter">voltmeter</a>). Another would be the errors of a well-fit predictive model. For instance, when I was an undergraduate I fit a model to predict the pitch, yaw, roll, and other attributes of an autonomous airplane. The results are below, and all closely follow a normal distribution:</p><p align="center"> <img src="https://bounded-regret.ghost.io/content/images/2021/11/airplane.png" width="200" /></p><p align="center"> <img src="https://bounded-regret.ghost.io/content/images/2021/11/airplane_errors.png" width="400" /></p><p>Why do well-engineered systems have normally-distributed errors? It's a sort of reverse central limit theorem: if they didn't, that would mean there was one large source of error that dominated the others, and a good engineer would have found and eliminated that source.</p><p><strong>Brainstorming exercise.</strong> What are some other examples of random variables that you expect to be normally distributed?</p><p><strong>Caveat: normal distributions have thin tails.</strong> The normal distribution has very “thin” tails (falling faster than an exponential), and once we reach the extremes the tails usually underestimate the probability of rare events. As a result, we have to be careful when using a normal distribution for some of the examples above, such as heights. A normal distribution predicts that no women should be taller than 6'8”, yet there are many women who have reached this height (read more <a href="https://www.johndcook.com/blog/2008/07/20/why-heights-are-not-normally-distributed/">here</a>).</p><p>If we care specifically about the extremes, then instead of the normal distribution, a distribution with heavier tails (such as a <a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution"><em>t</em>-distribution</a>) may be a better fit.</p><h2 id="log-normal-distributions"> <a href="#log-normal-distributions" class="anchor-heading" aria-labelledby="log-normal-distributions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Log-normal Distributions</h2><p>While normal distributions arise from independent <strong>additive</strong> factors, log-normal distributions arise from independent <strong>multiplicative</strong> factors (which are often more common). A random variable $X$ is log-normally distributed if $\log(X)$ follows a normal distribution–in other words, a log-normal distribution is what you get if you take a normal random variable and exponentiate it. Its density is given by</p><p>$p(x) = \frac{1}{x\sqrt{2\pi\sigma^2}} \exp\Big(-\frac{(\log(x) - \mu)^2}{2\sigma^2}\Big)$.</p><p>Here $\mu$ and $\sigma$ are the mean and variance of $\log(X)$ (not $X$).</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Examples of log-normal distributions<th style="text-align: center">Log-normal(0, 1) compared to Normal(0, 1)<tbody><tr><td style="text-align: center"><img src="https://bounded-regret.ghost.io/content/images/2021/11/lognormal_examples-1.png" alt="lognormal_examples-1" /><td style="text-align: center"><img src="https://bounded-regret.ghost.io/content/images/2021/11/standard-normal-lognormal-pdfs.jpeg" alt="standard-normal-lognormal-pdfs" /></table></div><p>Multiplicative factors tend to occur whenever there is a “growth” process over time. For instance:</p><ul><li>The number of employees of a company 5 years from now (or its stock price)<li>US GDP in 2030</ul><p>Why should we think of factors affecting a company's employee count as multiplicative? Well, if a 20-person company does poorly it might decide to lay off 1 employee. If a 10,000-person company does poorly, it would have to lay off hundreds of employees to achieve the same relative effect. So, it makes more sense to think of “shocks” to a growth process as multiplicative rather than additive.</p><p>Log-normal distributions are much more heavy-tailed than normal distributions. One way to get a sense of this is to compare heights to stock prices.</p><div class="table-wrapper"><table><thead><tr><th> <th>Height (among US adult males)<th>Stock price (among S&amp;P 500 companies)<tbody><tr><td>Median<td>175.7 cm<td>$119.24<tr><td>99th percentile<td>191.9 cm<td>$1870.44</table></div><p>To check if a variable X is log-normal distributed, we can plot a histogram of log(X) (or equivalently, plot the x-axis on a log scale), and this should be normally distributed. For example, consider the following plots of the Lognormal(0, 0.9) distribution:</p><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Standard axes<th style="text-align: center">Log scale x-axis<tbody><tr><td style="text-align: center"><img src="https://bounded-regret.ghost.io/content/images/2021/12/lognorm.png" alt="lognorm" /><td style="text-align: center"><img src="https://bounded-regret.ghost.io/content/images/2021/12/lognorm_xlog-2.png" alt="lognorm_xlog-2" /></table></div><p><strong>Brainstorming exercise.</strong> What are other quantities that are probably log-normally distributed?</p><h2 id="power-law-distributions"> <a href="#power-law-distributions" class="anchor-heading" aria-labelledby="power-law-distributions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Power Law Distributions</h2><p>Another common distribution is the power law distribution. Power law distributions are those that decrease at a rate of $x$ raised to some power: $p(x) = C / x^{\alpha}$ for some constant $C$ and exponent $\alpha$. (We also have to restrict $x$ away from zero, e.g. by only considering $x &gt; 1$ or some other threshold.)</p><p><img src="https://bounded-regret.ghost.io/content/images/2021/12/powerlaw-vs-lognorm.png" alt="powerlaw-vs-lognorm" /></p><p>Like a log-normal distribution, power laws are heavy-tailed. In fact, they are even heavier-tailed than log-normals. To identify a power law, we can create a log-log plot (plotting both the x and y-axes on log scales). Variables that follow power laws will show a linear trend, while log-normal variables will have curvature. Here we plot the same distributions as above, but with log scale x and y axes:</p><p><img src="https://bounded-regret.ghost.io/content/images/2021/12/powerlaw-vs-lognorm-loglog.png" alt="powerlaw-vs-lognorm-loglog" /></p><p>In practice, log-normal and power-law distributions often only differ far out in the tail and so it isn't always easy (or important) to tell the difference between them.</p><p><strong>What leads to power law distributions?</strong> Here are a few real-world examples of power law distributions (plotted on a log-log scale as above):</p><div class="table-wrapper"><table><thead><tr><th>Words in TV scipts<th>Words in the Simpsons<th>US city populations<tbody><tr><td><img src="https://bounded-regret.ghost.io/content/images/2021/11/power_law_words.png" alt="power_law_words" /><td><img src="https://bounded-regret.ghost.io/content/images/2021/11/power_law_simpsons.png" alt="power_law_simpsons" /><td><img src="https://bounded-regret.ghost.io/content/images/2021/11/power_law_cities.png" alt="power_law_cities" /></table></div><p>The factors that lead to power law distributions are more varied than log-normals. For a good overview, I recommend <a href="https://www.eecs.harvard.edu/~michaelm/postscripts/im2004a.pdf">this</a> excellent paper by Michael Mitzenmacher. I will summarize two common factors below:</p><ul><li><p>One reason for power laws is that they are the unique set of <strong>scale-invariant</strong> laws: ones where $X$ and $2X$ (and $3X$) all have identical distributions. So, we should expect power laws in any case where the “units don't matter”. Examples include the net worth of individuals (dollars are an arbitrary unit) and the size of stars (meters are an arbitrary unit, and more fundamental physical units such as the <a href="https://en.wikipedia.org/wiki/Planck_length">Planck length</a> don't generally affect stars).</p><li><p>Another common reason for power laws is preferential attachment or <strong>rich get richer</strong> phenomena. An example of this would be twitter followers: once you have a lot of twitter followers, they are more likely to retweet your posts, leading to even more twitter followers. And indeed, the distribution of twitter followers is power law distributed:</p></ul><p align="center"> <img src="https://bounded-regret.ghost.io/content/images/2021/11/number_of_followers_histogram.png" width="400" /></p><p>“Rich get richer” also explains why words are power law distributed: the more frequent a word is, the more salient it is in most people's minds, and hence the more it gets used in the future. And for cities, more people think of moving to Chicago (3rd largest city) than to Arlington, Texas (50th largest city) partly <em>because</em> Chicago is bigger.</p><p><strong>Brainstorming exercise.</strong> What are other instances where we should expect to see power laws, due to either scale invariance or rich get richer?</p><p><strong>Exercise.</strong> Interestingly, in contrast to cities, country populations do not seem to fit a power law (although they could fit a mixture of two power laws reasonably):</p><p align="center"> <img src="https://bounded-regret.ghost.io/content/images/2021/11/power_law_countries.png" width="250" /></p><p>Can you think of reasons that explain this?</p><p>There is much more to be said about power laws. In addition to the <a href="https://www.eecs.harvard.edu/~michaelm/postscripts/im2004a.pdf">Mitzenmacher paper</a> mentioned above, I recommend <a href="https://terrytao.wordpress.com/2009/07/03/benfords-law-zipfs-law-and-the-pareto-distribution/">this</a> blog post by Terry Tao.</p><p><strong>Concluding Exercise.</strong> Here are a couple examples of data you might want to model. For each, would you expect its distribution to be normal, log-normal, or power law?</p><ul><li>Incomes of US adults<li>Citations of papers<li>Number of Christmas trees sold each year</ul></div></div><div class="search-overlay"></div></div>
