{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "palestinian-witness",
   "metadata": {},
   "source": [
    "# Homework 5 Lab: Forecasting Points Per Game (PPG) Part II\n",
    "\n",
    "As in Homework 4 Lab, we will be using data scraped from https://www.basketball-reference.com/ to explore different forecasting strategies to predict who will have the highest PPG in the NBA in a particular week. We'll use data from the 2020-21 NBA season between December 2020 and March 2021 to forecast the highest PPG players in April 2021, the final month of the regular season. Part I focused on the **reference class forecasting** approach. Here we will explore a different approach, **decomposition into parts**. This lab has many open-ended questions, and there is often not one correct answer. We will grade submissions mostly based on your reasoning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-mason",
   "metadata": {},
   "source": [
    "## Gradescope Submission \n",
    "To submit this assignment, rerun the notebook from scratch (by selecting Kernel > Restart & Run all), and then print as a pdf (File > Save and export notebook as... > Webpdf) and submit it to Gradescope. After downloading,  **please double-check that the pdf you generated contains all of your work, including any relevant plots.** You are also welcome to use R or another language if you are more comfortable with data analysis in that language.\n",
    "\n",
    "\n",
    "**This assignment should be completed and submitted before Tuesday, February 20, 2024 at 11:59pm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-prior",
   "metadata": {},
   "source": [
    "### Data\n",
    "We are using the same raw data as in the previous lab, which is in the file \"scraped_games_2020-21.csv\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"scraped_games_2020-21.csv\", index_col=0, parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-dividend",
   "metadata": {},
   "source": [
    "### Optional preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-broad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into pre-April and April data; here we'ved named them train and test but the variable names don't matter.\n",
    "df_train = df[df[\"Date\"] < \"2021-3-29\"]\n",
    "df_test = df[df[\"Date\"] >= \"2021-3-29\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We group the games by week and find the average points scored by each player that week\n",
    "# The flag freq='W-MON' will group data weekly on Mondays as described in https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases\n",
    "df_train_weekly = df_train.groupby([pd.Grouper(key='Date', freq='W-MON'), 'Player'])['Pts'].mean().reset_index().sort_values(['Date', 'Pts'], ascending=[True,False])\n",
    "df_test_weekly = df_test.groupby([pd.Grouper(key='Date', freq='W-MON'), 'Player'])['Pts'].mean().reset_index().sort_values(['Date', 'Pts'], ascending=[True,False])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-liquid",
   "metadata": {},
   "source": [
    "Note that the above pre-processing is different from Part I's pre-processing. The above uses mean() rather than sum(), to get average points rather than total points in a week (since there might be different numbers of games each week). There are also other ways to get average points per game (that you might have used in Part I), but this is one such way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-major",
   "metadata": {},
   "source": [
    "# Approach: Forecasting by decomposition\n",
    "\n",
    "Last week we explored reference class forecasting. A different approach is to break down the outcome in which we're interested into its component parts and analyze these parts. For this PPG question, we can calculate each player's average PPG over a week and their standard deviation. Then we can fit a distribution to this average and standard deviation and by sampling, predict the probability that each player will have the highest PPG in a future week.\n",
    "\n",
    "For example, we can find how many total points Stephen Curry scored in each week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_weekly[df_train_weekly.Player == \"Curry,Stephen\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-mobility",
   "metadata": {},
   "source": [
    "We can plot a histogram of the point distribution and visualize that it is pretty noisy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-discovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Stephen Curry weekly points\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.hist(df_train_weekly[df_train_weekly.Player == \"Curry,Stephen\"]['Pts'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-russian",
   "metadata": {},
   "source": [
    "Some of this noise comes from the fact that there are different numbers of games being played each week, but after taking that into account there is still probably significant variation in points per game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-gnome",
   "metadata": {},
   "source": [
    "## Q1. Using a Gaussian approximation\n",
    "\n",
    "For at least 10 players that seem like contenders for getting the highest PPG (you choose the players, and you can also do this for more than 10 players):\n",
    "\n",
    "a) Compute their mean weekly-points-per-game and standard deviation.\n",
    "\n",
    "b) Approximate each player's weekly-points-per-game with an independent Gaussian distribution. Using these Gaussian approximations, calculate or simulate each of the players' probabilities of getting the highest PPG in a future week. (i.e., make a forecast that assigns a probability $q_i$ to each player)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-nickel",
   "metadata": {},
   "source": [
    "## Q2. Modeling other players\n",
    "Modify your forecast in Q1 to also forecast a probability for other players in the NBA. For example, some considerations you might think about include: are the players you ranked 9th and 10th very similar to 2, or 10, or 50 other players in the NBA, and should they split the probability density equally?; which players have a vanishingly small probability of getting the highest PPG?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-arabic",
   "metadata": {},
   "source": [
    "## Q3. Evaluate the Gaussian approximation forecasts\n",
    "\n",
    "Calculate the Brier quadratic score for the forecasts from Q1 and Q2. How do they compare to the Brier scores from the previous lab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-poison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-customer",
   "metadata": {},
   "source": [
    "## Q4.  Beyond Gaussians\n",
    "\n",
    "a) For (at least) the 10 players in Q1 (but more is fine, too), plot a histogram of their weekly points like in the Stephen Curry example above, and superimpose the Q1 Gaussian distribution approximation on the histogram. (i.e., plot the Gaussian probability density function based on the mean and standard deviation calculated for that player). Note that you may need to scale the histogram units and/or the Gaussian pdf in order to get the histogram and the Gaussian pdf to look comparable.\n",
    "\n",
    "b) Pick (at least) 2 players whose point distributions do not fit the Gaussian approximation well. Propose different distributions to approximate the data, and plot these curves on the histograms. Discuss why you chose these distributions, and if you fit the parameters of the distribution to the data, describe how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-carter",
   "metadata": {},
   "source": [
    "## Q5. Independence assumptions\n",
    "\n",
    "a) So far in this decomposition approach, we have assumed that each player's points are independent of another's. What are some things that might cause points in a game to not be independent between players?\n",
    "\n",
    "b) What are some ways you might quantify and model these dependencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
